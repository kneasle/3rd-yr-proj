\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{fancyvrb}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{wrapfig}

\title{Insert Report Title Here}
\author{Benjamin White-Horne \\ \emph{Oxford University}}

\newcommand{\row}[1]{\texttt{#1}}
\newcommand{\br}[0]{\vspace{10pt} \noindent}

\begin{document}

% Title page & T.o.C.
\maketitle
\pagebreak

\tableofcontents
\pagebreak

\section{Overview of Change Ringing}

This project is to build a tool to help composers design complex sequences of ringing of church
bells.  Change ringing is a very niche and varied activity so this section will serve as a quick
introduction to the topic and overview of the aspects of change ringing which are most relevant to
this project.  A more complete overview can be found
on Wikipedia\footnote{\url{https://en.wikipedia.org/wiki/Change_ringing}}.

\br{}Change ringing (sometimes known as full-circle church bell ringing) originated in English
churches in around 1600, where churches began hanging sets of tuned bells on wheels in towers.
The installations range from 3 to 16 bells, with even numbers from 6 to 12 being almost ubiquitous.
These bells are typically quite heavy (bells over a ton are fairly common) and are spun
back and forth through 360 degrees like a pendulum.  All the bells are rung once in a sequence which
changes each time.  A piece of change ringing is a sequence of permutations, known as `changes'
(hence `change ringing').  Bells only move by at most one place between two adjacent rows.  On
paper, we write these permutations in order as rows of numbers and by convention, the bells
are numbered sequentially with `1' referring to the highest-pitched bell in the tower.  For
example:

\begin{verbatim}
12345678
21436587
21345678
12436587
12345678
\end{verbatim}

\subsection{Methods}
A performance of ringing will often contain thousands of individual rows.  Physical aids to memory
are not permitted during performances so the ringers must ring thousands of unique rows completely
from memory.  Memorising them all directly is clearly not feasible, so compositions are built up
from repeating patterns known as `methods'.

A method has a name (often with historical or geographical connections). In the method each bell
moves through a pre-determined path, moving back and forth within the sequence. Ringers learn this
path to help them remember where to ring in the sequence of rows.

\begin{wrapfigure}{r}{0.2\textwidth}
\centering
\begin{BVerbatim}
12345678
21436587
24163857
42618375
46213857
64128375
61482735
16847253
--------
16482735
61847253
68174523
86715432
87614523
78165432
71856342
17583624
--------
17856342
\end{BVerbatim}
\caption{Two leads concatenated}\label{fig:two-leads-lb8}
\end{wrapfigure}

A single instance of a method is called a `lead' and can be thought of as a super-permutation; it
takes a start row and produces a sequence of rows, ending with the row which the next lead should be
started (rows like this are not considered part of the super-permutation).

For example, the following are all leads of the same method\footnote{This particular method is known
as `Little Bob Major'.} (we write the `leftover' row under a dashed line because it belongs to the
lead \emph{after} the others):

\begin{multicols}{3}

\centering
\begin{BVerbatim}
12345678
21436587
24163857
42618375
46213857
64128375
61482735
16847253
--------
16482735
\end{BVerbatim}

\centering
\begin{BVerbatim}
16482735
61847253
68174523
86715432
87614523
78165432
71856342
17583624
--------
17856342
\end{BVerbatim}

\centering
\begin{BVerbatim}
12486753
21847635
28174365
82713456
87214365
78123456
71832546
17385264
--------
17832546
\end{BVerbatim}

\end{multicols}

Note that a single lead of this method preserves the location of the bell at the start of the
row.  This is an extremely common feature of methods because it allows ringers to orient
themselves around this `fixed' bell and therefore helps prevent mistakes.

One observation we can make here is that the second column begins with the leftover row of the
first.  Therefore, we could concatenate the two leads together to form a longer piece (as in
fig~\ref{fig:two-leads-lb8}). Note how the dashed lines split consecutive leads.

\subsection{Compositions}

A `composition' is a sequence of rows of some length which satisfies the adjacency property and
starts and finishes with the row containing bells in descending order (\row{123456\ldots}, known as
`rounds').  Thus, the above diagram shows a valid composition.

The task of a composer is to create compositions which have some set of desirable features (which is
likely to depend heavily on the circumstances of each performance).  In order to understand the
rationale behind this project and gauge its success, we must first understand some of the most
important features or constraints of compositions.

\subsection{Features of Compositions}

\subsubsection{Truth/Falseness}

A composition is called \emph{true} if every row is unique, and is called \emph{false} otherwise.
This is \emph{the} most important property that a composition must have, since any performance of a
false composition is considered invalid and therefore a waste of the ringers' time.  When we write
out compositions we usually write out rounds twice, but when considering falseness it is only
counted once (thus, rounds in the middle of a composition is still false).

\subsubsection{Length}

The `length' of a composition is the number of rows which it contains.  Like falseness, external
rounds is only considered once.  90\% of performances have one of two length ranges:

\begin{enumerate}
    \item A \textbf{Peal} consists of at least 5000 rows (about 3 hours' of ringing).
        The number 5000 is chosen because historically the pinnacle of ringing challenge was to ring
        every permutation of 7 bells, which results in $7! = 5040$ rows.  This was recently rounded
        down to 5000 for all numbers of bells, but the majority of people prefer the completeness of
        5040 rows for peals of 5, 6 or 7 bells (where every possible row is rung the same number of
        times).
    \item A \textbf{Quarter Peal} consists of at least 1250 rows (a quarter of a peal's length, and
        about 45 minutes of ringing).  Due to the reduced completion time, these are much more
        common than peals.  Half peals ($\ge 2500$ rows) and double-length peals ($\ge 10,000$ rows)
        are both recognised lengths but performances of them are very rare.
\end{enumerate}

Also, every row above the lower bound is extra ringing time for little gain in performance merit,
and therefore composers try to make compositions which are fairly tight to these lower bounds.
Indeed, lengths within the ranges $1250 \le l < 1300$ or $5000 \le l < 5100$ are almost ubiquitous.

\subsubsection{Music}

If we are composing for $\ge 8$ bells, then our composition will not contain every possible row and
we therefore have the opportunity to decide which rows get included and which don't.  Of these, some
may be more favourable than others --- in general ringers prefer rows with low entropy over those
which sound more random.

Some common forms of music are as follows (examples are given on 8 bells, but logical extensions
usually exist on other numbers).  By convention I will use \row{x} as a wild card for any bell (like
\verb"." in regexes):

\begin{enumerate}
    \item \textbf{Runs of bells}: Rows which start or finish with $\ge 4$ consecutive bells.
        The exact bells are unimportant and the longer the run the better.  For example
        \row{345678xx} starts with a descending run of 6 bells, and is therefore preferable to
        \row{xxxx4321} which ends with an ascending run of 4 bells.
    \item \textbf{Named rows}: Single rows which people deem especially pleasing.  Some examples:
        \begin{itemize}
            \item Queens: \row{13572468} --- odds then evens
            \item Backrounds: \row{87654321} --- rounds but reversed
            \item Tittums: \row{15263748} --- alternates high and low, sounds like `tee-tum-tee-tum'
        \end{itemize}
    \item \textbf{Musical patterns}: Where part of the row forms an interesting pattern.  For
        example \row{xxxx6578} (known as `65s') or \row{x5x6x7x8} (from Tittums).
\end{enumerate}

\subsubsection{Complexity}

Complexity does not have a nice mathematical definition and therefore this project will make no
attempt to automate it.  However, it is still an important concept which roughly corresponds to how
repetitive and predictable is your composition.

In general, we want to make these things as simple as possible because that will open your
composition to the most potential bands.  However, sometimes \emph{the point} of a composition is to
be a challenge, in which case complexity is fair game.

\subsubsection{Summary}

In general, length and falseness are hard constraints which composers have little choice over and
must work around.  Most of the time, the composer fixes some target level of complexity and tries
to get as much music or interest as possible within that constraint.  Also, many composers make
compositions that they intend to conduct themselves, allowing for a higher level of complexity
because they already have an intimate knowledge of the composition when learning it.



\pagebreak

\section{Project Goals}

This project concerns prototyping an application to \emph{aid} composers whilst requiring the
smallest possible change to their existing workflow.  This is analogous to what Microsoft Word
provides for writers --- a kind of `automated paper' which automatically annotates your work with
data that is tedious to compute manually whilst still providing it in a format that you are familiar
with (i.e.\ words on a page).

There is currently nothing like this in the composing ecosystem, which I believe is a bad omission.

\subsection{Specific Aims}

Whilst this goal is good for general direction, we need more specific goals in order to build a
cohesive application.  Therefore, I will split up this general goal into simpler goals which can
easily be used to rate the resulting application.  Most of these come from observations of existing
programs, conversations with well-known composers, and experience gained from experimentation early
in the project.

\paragraph{Ease of Use} The application should be made so that anyone can install and use it easily,
without any technological knowledge or a lengthy installation process.

\paragraph{Completeness} There is a concrete definition of what is considered `Change
Ringing'\footnote{The definition is defined by the Framework for Method Ringing, found here:
\url{https://cccbr.github.io/method_ringing_framework/classification.html}}, and the states
representable in the application must be a superset of this definition.

\paragraph{Incremental} The application should allow the user to incrementally build up their
composition, in whatever order they want, starting wherever they want to.

\paragraph{Instant} Feedback on important measures like truth, music content and length should
always update instantly whenever the composer changes their composition.

\paragraph{Visual} Everything that can be understood visually should be displayed visually.  Where
concrete numbers are required (e.g.\ length), these should also be provided.

\paragraph{View-Independent} The application should allow the user to choose how they view the
composition they are working on (and this should be configurable whilst midway through a
composition).  (N.B. I actually didn't do this, so it might be worth removing it?)



\pagebreak

\section{What has been done so far?}

In this section, I will go through existing tools and review them against the goals set above.
These examples also influenced the core design decisions of the resulting prototype.

\subsection{Composition Library}

Composition Library\footnote{Found at \url{https://complib.org/}} (usually
known as CompLib) is not trying to be a composition editor, but instead is an attempt to build a
complete central library of all known compositions.

However, it does have a composition editor in order to allow people to add new compositions to the
library.  Despite not being designed with experimentation in mind, this editor is good enough that
I think it's probably the best composing tool out there right now.  I have made several
compositions using only CompLib, paper and a pencil for assistance.  More than anything, I think the
fact that the best tool right now isn't even designed for the purpose highlights how much promise a
custom-designed composing tool holds.

\subsubsection{Pros of CompLib}

Firstly, CompLib nails \emph{ease of use}.  It is simply a web application which works across all
browsers and therefore requires no installation (assuming that most users would need a browser to
install software).  It does need an account in order to add compositions, but in return it stores
all your work (published or private) on its server which can then be accessed anywhere.

\paragraph{Incremental:}  CompLib can't handle partial compositions at all; compositions must start
(and preferably end) in rounds.  If the composition does not come round (as is the case for almost
all unfinished compositions), then CompLib generates thousands of repeated rows on the end of your
composition in the hope that the comp will come round, to the detriment of performance and
usability.

\paragraph{Instant:}  CompLib does reasonably well.  If the composition is a round block, then it
gives a good visual indication of where the falseness occurs.  However, it isn't perfect.  The main
problem is that all the row generation and proving is handled server-side, which means that any
change to the composition has to travel across the internet before the user gets any feedback.
Also, there is a related concurrency bug where making too many changes in quick succession will
cause some of them to be overwritten and lost.

\paragraph{Visual \& View-Independence:}  CompLib again does reasonably well (and it is flawed only
because it is designed for inputting and later learning already finished compositions).
CompLib allows the user to choose between a range of several ways of viewing their composition (even
when it doesn't come round), and all of these are very easy to read.  On the other hand, the
blue line view doesn't highlight musical rows, which I think would be beneficial even for CompLib's
use case of people learning comps.  Also, if you want to see more detail of one segment of a
composition, there is no way to `unfold' only part of it --- you have to switch to an entirely new
view and then find that location again.

\begin{enumerate}
    \item If the composition is complete, CompLib has a really nice music breakdown available to see
        if your goals for the composition has been met.  However, this is only available for
        complete compositions and viewing it requires saving and reopening the composition which is
        not ideal.
    \item CompLib's input format is very declarative --- in essence, the user enters the composition
        as a sequence of instructions for how to build the rows and these instructions are read
        in order and used to generate (`prick') the rows of your composition.  These rows are then
        used to calculate the falseness of your composition, and what will be displayed to you.
        This, however, is frustrating when you're experimenting with compositions, since a change
        near the top of a composition will cause different rows to be generated for the rest of the
        composition.  This behaviour makes perfect sense because CompLib's editor is designed for
        inputting a composition which you already know (not for experimentally building up a new
        composition) but is nonetheless makes experimentation frustrating.
\end{enumerate}

\subsection{Inpact}

Inpact is a program written by Alexander Holroyd to support the work-flow of experimenting with
compositions.  It provides instant feedback on truth and music, but is visually quite clunky and
doesn't support `partial' compositions.

\paragraph{Ease of use:}

\paragraph{Incremental:}

\paragraph{Instant:}

\paragraph{Visual \& View-Independence:}

\subsubsection{Things that Inpact does well}

\begin{enumerate}
    \item It has a folding interface --- the user can fold away sections of the composition in order
        to get a higher-level overview of the composition as a whole.
    \item It doesn't require that your composition is complete before giving feedback about truth or
        music content.
    \item It has customisable music scoring --- the user decides what music they care about.
\end{enumerate}

\subsubsection{Things that Inpact doesn't do well}

\begin{enumerate}
    \item Doesn't handle `partial' compositions --- compositions have to be a single block starting
        at rounds, but don't have to come round.
    \item There isn't a visual way to see the music in the composition.
\end{enumerate}



\pagebreak

\section{Initial Design Decisions}

Now that we've taken a look at the state of the art, it's time to start making design decisions
about this project.  To do this, I will lay out my initial design and why I think it will satisfy
each of the design goals.

\subsection{Technologies Used}

When choosing technologies for a project, we must first decide what the project will require of
them.  For this project, we will need to maintain a pretty-looking canvas-style user interface which
can be updated in real time if the user were to pan the viewport or drag parts of the composition.
However, behind the scenes, the application will have to perform a large amount of permutation logic
every time the user changes anything and must do all of this without a noticeable delay or hitch to
the UI, regardless of the size and complexity of the composition.  This logic will be fairly complex
and its correctness is vital for the application to work as expected.  Finally, the application
should be as easy to get running as possible --- i.e.\ we want to minimise the time between the user
discovering that this application exists and them starting to use it.

\subsubsection{Application as a Static Web Page}

Therefore, I propose that the application run as a single static web page (i.e.\ it is a fixed set
of files that can be served by a simple HTTP server).  This has many advantages:
\begin{enumerate}
    \item There is no start-up time for a new user --- they open the page in any browser and the
        application starts immediately.
    \item Having no dependence on a server both obviates the need for the user to make an account
        and also makes it trivial to deploy using GitHub pages or a similar service.
    \item Everyone is always using the most up-to-date version of the code unless they build a
        specific version from source themselves.  This is not to be taken for granted --- I have
        learned from releasing other projects into production that people rarely update your code on
        their own accord.
    \item As shown by CompLib, running application logic on a separate server produces a marked drop
        in user experience.  Running all the logic on the same computer means that concurrency bugs
        and network delay are both eliminated.
    \item HTML canvas rendering (while extremely inefficient compared to OpenGL or Vulkan) is easy,
        fast enough to feel instant and consistently looks good an all browsers and operating
        systems.  As of the first prototype, the performance of naively redrawing the whole canvas
        every update is more than adequate (at least on my brand-new PC).  However, if performance
        does become an issue there are many obvious caching optimisations that would cause huge
        speed-ups in rendering.
\end{enumerate}

\br{}However, static web pages come with their own challenges.

\begin{enumerate}
    \item None of the code can run server-side so the entire application must run entirely within
        browsers.  The obvious language here is JavaScript since it is reliably supported by every
        browser, but JavaScript is in no way well-suited to writing complex applications.  It lacks
        any static typing which makes complex code difficult to write and maintain.  It also has
        very unpredictable runtime performance (being entirely dependent on JIT compilation),
        particularly when doing operations which involve handling large amounts of data in complex
        ways (which this project relies heavily on).
    \item Storing persistent data in a cross-browser format is very challenging.  Cookies are an
        option but they are sent to the server with every HTTP request, making them unsuitable for
        storing large save states.
\end{enumerate}

\subsubsection{Move Complex Logic to Rust and WebAssembly}

My solution to both of these is to write the code with a client/server architecture, where there is
a thin client written in JavaScript (to handle user interaction and rendering) and the server is
written in Rust and compiled to WebAssembly.  There are many other languages that could work for
this purpose, for example TypeScript, C/C++ or Go but I chose Rust for the following reasons:

\begin{enumerate}
    \item Rust gives you direct control over memory layout which provides excellent performance when
        manipulating complex data structures.  The performance of TypeScript, for instance, is by
        definition no better than that of JavaScript.
    \item The Rust compiler performs a large amount of strict compile-time correctness checks on
        your code, making it easy to implement complex algorithms in ways that you know is correct
        and fast.  C/C++ and Go cannot compete in this regard.
    \item Rust's WebAssembly support is fantastic, with particular shout-out to \verb|wasm-bindgen|,
        which generates binding code between native Rust and native JavaScript to let them interface
        seamlessly.
    \item I am already proficient in Rust, and I can share code between ringing-related projects to
        save development time.
\end{enumerate}

\subsubsection{Using Cookies as Persistent State}

Fortunately for this project, there is some storage that web pages can use to store persistent data
without the user's input, and those are cookies.  Unfortunately, cookies are severely size limited
--- a web page can only expect to be able to save about 30 cookies per page, each of which can be at
most 4096 bytes, so we have 122,800 bytes in which to save all the persistent state we need.  In
reality, however, it's not a good idea to run too close to this limit because the page may have to
store other cookies and these shouldn't suddenly fail to save.

Saving just the current undo snapshot is fine, but I want to save the undo history which will
require a complicated serialisation/deserialisation algorithm.  I have chosen not to implement this
for the prototype, because I suspect that I will need to change the internal representation a lot
and doing so will break any serialised data.

\subsection{The Application Model}

This is the high-level model that should define how the application should behave (even if the
implementation details are different).  This model contributes to most of the learning curve for new
users and therefore should be as simple as possible without losing expressivity.  I'm going to
describe the model in terms of Haskell data types, using names identical to those used in the
project's source code (the actual implementation will look largely similar due to Rust being heavily
inspired by Haskell).

\subsubsection{Specification vs Derived State}

In order for the model to be as simple as possible, it is important to differentiate the
\emph{specification} from any \emph{derived state} generated from it (in the code, these are stored
in structures called \verb|Spec| and \verb|DerivedState| respectively, both in the \verb|jigsaw/|
crate/directory).

As a toy example, let us consider two integers ($a$ and $b$) and their sum and product ($a + b$ and
$ab$) as the total information required to represent a particular state.  This overall state
consists of 4 pieces of information ($\{a, b, a + b, ab\}$) but it's easy to see that the inclusion
of $a + b$ and $ab$ does not add any extra information.  Therefore, we can say that $\{a, b\}$ is
the \emph{specification} of $\{a, b, a + b, ab\}$, whereas $\{a + b, ab\}$ is the \emph{derived
state}.  This dependence also means that we don't have to store any derived state in the undo
history, drastically reducing the memory footprint of storing long undo histories (provided that the
derivation process is not prohibitively expensive --- though in the case of this project it is quite
cheap in both time and memory).  Plus, the mental load on the user is reduced because they only have
to reason about the specification and the behaviour of the derived state should be intuitive given
the changes to the specification.

\subsubsection{High-Level Requirements of the Model}

For this project, we have a few requirements which will constrain the model used (namely the
completeness and generality goals).  Firstly, to satisfy generality the model must be able to
express as many partial compositions as possible.  From discussions with other composers on this
topic, I know that it is a very common work-flow to begin a composition with ideas that fit into
three broad categories:

\begin{enumerate}
    \item \textbf{Meta-data:} Some concept of overall structure, for example which method(s) to use,
        what part structure to use, etc.  These are usually laid out before composing starts, and
        it is likely that the finished composition will have the same meta data as the initial idea.
    \item \textbf{Fragments:} Some chunks of the composition that they want to ring, perhaps with
        some idea on what order these should be rung in (no assumptions are made about whether or
        not these ``fragments'' are connectible or even mutually true).  These will almost certainly
        have to be extended and joined in order to reach a final composition, and will therefore be
        modified extensively over the composing process.
    \item \textbf{Abstract} ideas about complexity, use of repeated blocks, etc.\ which are
        relevant to the resulting composition but extremely difficult to represent in a
        machine-compatible way.  This model will not attempt to encapsulate these ideas, since they
        are so varied, abstract and complex that a sufficiently general model would be enormously
        complex for both the programmer and the user.  Also, these `rules' are usually strategically
        bent by composers due to context that isn't available to the computer so having an
        application that enforces them would likely feel overly restrictive.
\end{enumerate}

\subsubsection{Model Datatypes}

Here, I will describe a model around which this project will be built.  As described above, we will
only model the first two categories of ideas described above, as they are the most important and
straightforward to implement.  I will describe the model as Haskell-style data types, using some
light syntax extensions for things like \verb+{| .. |}+ for counted sets (a.k.a.\ bags).  For
consistency between this report and the code, I have given data types the same
names as their counterparts in the Rust source code wherever possible.

First of all, a \verb|Spec|ification of a composition is a combination of a bag of \verb|Frag|ments
coupled with some \verb|MetaData| (in the code, the meta-data is simply flattened into the
\verb|Spec| structure):

\begin{verbatim}
data Spec = Spec {
    frags :: {| Frag |},
    meta_data :: MetaData
}
\end{verbatim}

In this project, we will model fragments simply as an ordered sequence of annotated rows.  These
`annotations' encode the higher-order structure of the composition in a view-independent format,
so that the same composition can be displayed to the user in whatever way they prefer to view
compositions (thus the exact calling of the composition becomes \emph{derived state}):

\begin{verbatim}
type Frag = [AnnotRow]

data AnnotRow = AnnotRow {
    row :: Row,
    is_rule_off :: Bool,
    call_str :: Maybe String,
    -- more annotations omitted for brevity ...
}
\end{verbatim}

The \verb|MetaData| contains the remaining information required to specify the composition,
including part heads and the composition's stage:

\begin{verbatim}
data MetaData = MetaData {
    part_heads :: [Row],
    stage :: Stage
}
\end{verbatim}

The remaining data types in this definition (\verb|Row|, \verb|Stage|, \verb|Bell|, etc.) are stored
in a separate library to the project's business code in the \verb|core| directory (imported as
\verb|proj_core| in the code).  For completeness, they could be defined in Haskell as follows:

\begin{verbatim}
type Row = [Bell]

data Bell = Bell Int
data Stage = Stage Int
\end{verbatim}

\subsubsection{Operations on the Data}

Now that we know how a composition is specified, we now need to define how the user will be able to
modify the data types defined above.

(T.B.C.)



\pagebreak

\section{The Trials and Tribulations of Actually Coding this Thing}

\subsection{Facebook Poll}

(Dec 3, Poll linked
below\footnote{\url{https://www.facebook.com/groups/601658156640471/permalink/2097786800360925}})

Having lain down a rough initial design, I wanted feedback from other composers to determine if it
would be widely useful.  I am self-taught in composing, and it's entirely possible that I have a
completely non-standard workflow.

To achieve this, I posted an open question on a Facebook group that I know is frequented by other
composers asking (in general terms) what their composing workflow was.  I got a good number of
responses (64 comments in total), and there was a surprising variety in approaches.  Some people
generate compositions entirely through computer search (which is out of scope for this project) but
most composers compose by hand.  Of these, nearly everyone said that they build up compositions from
small pieces and agreed that my design would be interesting and potentially useful.  (N.B. Possibly
put somethinghere about coursing orders/course heads, but I'm not sure if it's relevant).

\subsection{Starting with a Core Utility Library}

(mid-December, 9e2dba6 $\to$ 490cfa2)

When starting this project, it quickly became clear that I was going to need a core library of
common data types and functions.  Thus, the first thing I did was to lay down a basic version of
this library which would then increase the speed of development later down the line.

This initial version of the library had newtypes for \verb|Bell|, \verb|Row|, \verb|Perm|utation,
\verb|Stage| and \verb|Block|.  All of these could be parsed and formatted in a human-friendly way,
and use Rust's type system to enforce invariants (e.g.\ a \verb|Row| can't contain the same
\verb|Bell| twice).  Doing this allows most runtime checks to be removed, increasing the performance
of manipulating data at the cost of slightly slower creation or parsing.  Also, I put a lot of
effort into maintaining good documentation (including examples) for this library so it can be easily
used in other projects or by other developers.

\subsection{Talk with Anders Holroyd}

TODO

\subsection{Row vs Perm}

After taking with Anders, I thought that the concept of rows and permutations were different enough
to warrant separate data types.  My thinking was that a \verb|Row| is a sequence of \verb|Bell|s
(plus some additional invariants), and a permutation can be thought of as a function of type
\verb|[a] -> [a]| which can permute sequences of any type.  Mathematically this feels like an
elegant separation, but using the library in ernest made me realise that the \verb|Perm| doesn't
pull its weight given its large overlap with the \verb|Row| type.  I would almost always use
\verb|Perm| only as an intermediate step to use one \verb|Row| to permute another.  Therefore, a few
months into the project (Feb 9th, 81d1120) the operations of the \verb|Perm| structure was merged
into \verb|Row|.  This made the library much more ergonomic (if slightly less mathematically
elegant).

\subsection{Setting up Web Project}

After the Christmas vacation, I set to work getting Rust and JavaScript to work nicely together when
Rust is compiled to WebAssembly.  This is easier said than done, because an infamous limitation of
WebAssembly is that only integers and floating-point numbers can be passed over its API.\@  This is
severely limiting but sufficient since JavaScript and WebAssembly can share an address space and
thus pass pointers across the boundary and copy the data.

Creating binding code like this by hand is incredibly tedious and error-prone and one of the reasons
I chose Rust was because of
\emph{wasm-bindgen}\footnote{\url{https://github.com/rustwasm/wasm-bindgen}}, a tool which generates
this binding code automatically and requiring only minimal changes to the source code.  This means
that the hand-written JavaScript and Rust code can both use native types, and the fiddly API details
are taken care of.  There are still some limitations (mostly due to Rust and JavaScript having
radically different memory models) but after a few iterations I found a project architecture which
plays to \emph{wasm-bindgen}'s strengths and avoids its limitations.

I initially struggled to discover how to serve \verb|.wasm| files
without using NodeJS, but eventually I stumbled on a tutorial which allowed me to build the static
web page that I was after.  At this point, the build process involved running several commands in
sequence so I created a simple shell script (\verb|build.sh|) to run the full build in one command.
In order to publish the project to GitHub pages, this script was later improved and translated to
Python (April 27, d8d388b).

\subsection{Maintaining Undo History}

\subsection{Passing Data}

As the project progressed, it became clear that a large amount of relatively complex data had to be
communicated between JavaScript and Rust.  Even with \emph{wasm-bindgen}, the types that can be sent
over the language interface is limited to flat structures with no references.  This is unfortunate,
since the internal data structures have 3 layers of references (\verb|Spec| $\to$ \verb|Frag|s $\to$
\verb|Row|s $\to$ \verb|Bell|s or \verb|DerivedState| $\to$ \verb|DerivedFrag|s $\to$
\verb|AnnotRow|s $\to$ \verb|Bell|s).

Initially, I didn't want JavaScript and Rust to hold their own copies of the same data, since there
would be two potentially divergent sources of the truth.  Also, I wanted to limit the complexity of
the JavaScript code (due to its inferior performance and maintainability) so it made sense for Rust
to handle all the state and then expose a simple API to JavaScript to read or modify this data.

The modification part of the API provides one function per operation on the composition, which
enforces a separation of concerns because JavaScript has no knowledge of how Rust chooses to
implement these functions.  This worked excellently and I had no issues with it.

In order to work round the inherent issues of WebAssembly, the initial data read API was a flat set
of getter functions, looking like Figure~\ref{fig:initial_api}.

This worked fine for a while but as the complexity of the datatypes grew, the number of exported
functions exploded and refactoring the API became an unnecessary challenge.  Also, every WebAssembly
function call takes a non-trivial amount of time and this flat API encourages huge numbers of calls
each frame which started causing performance issues.  Also this data won't change each frame so we
may as well cache the data in a native JavaScript data structure.

It is worth noting here that the API is split 

\begin{figure}
\begin{verbatim}
/// Get the number of fragments
fn get_num_frags() -> usize { ... }

/// Get the number of rows in the fragment at a given index
fn get_frag_length(frag_index: usize) -> usize { ... }

/// Get the bell at a given location
fn get_row(
    frag_index: usize,
    row_index: usize,
    bell_index: usize
) -> Bell { ... }
\end{verbatim}
\caption{Initial getter API design}\label{fig:initial_api}
\end{figure}

\subsection{Project Architecture}

\subsection{Mute/Solo}

\subsection{Line Rendering}

\subsection{Implementing Folding}

- Bluelines
- Falseness ranges

\subsection{API Design of Core Library}

\subsection{Releasing Prototype to GitHub Pages}

- It's just sooooooo hard

\subsection{Final Stats}

As of writing this report, the project clocks in at 275 commits, 5,736 source lines of code
(including 4,233 of Rust, 1,033 of JavaScript, 240 of HTML, 114 of CSS and 85 of Python for the
build script).



\pagebreak

\section{Conclusion}

Blah Blah Blah\ldots



\pagebreak

\section{Glossary of Change Ringing Terms}

\paragraph{Stage:} The number of bells which the composition uses.  This is not necessarily the
number of bells used to ring a given composition, but for the purposes of this project the
distinction is not relevant.

\paragraph{Row:} A sequence of bells which forms a permutation.  Rows are the fundamental building
blocks of compositions.  The words `change' and `row' are often used interchangeably (hence the name
`Change Ringing'), but to avoid confusion I will always use `row' in this report.

\paragraph{Composition:} An ordered sequence of rows which tell ringers in which order the bells
should be rung.

\paragraph{Truth:} For the purposes of this project, a composition is \emph{`true'} if all the rows
are unique, otherwise it is \emph{`false'}.  This is the single most important property of a
composition, since any performance of a `false' composition is considered invalid.

\paragraph{Method:} A short, usually symmetrical, pattern that is repeated throughout a composition
with small and well-defined modifications (known as `calls').  This is the key to how human ringers
can ring over 5000 rows worth of composition without any memory aid --- in reality, they memorise
the method(s) and one `conductor' memorises the pattern of calls and calls them in the right places
for the other ringers to take effect.  It is usually the job of the composer to make sure that the
call sequence is predictable and easy to memorise.

\paragraph{Lead:} A single instance of the repeating pattern of a method.

\end{document}
